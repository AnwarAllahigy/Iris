# -*- coding: utf-8 -*-
"""AssigmentNo1-MachineLearning_v4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uk2uoG1QRwHdXgmaaWBJh3N4reY5UmIR

# **Programm No.1 Assigment1 of Machine Learning**
---By **Anwar Hashem** (Student No. 40234100001)
---Supervisor **Dr.Mansor Zadeh**

## **Importing Libraries**
--- here we importing the needed **Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set(style="white", color_codes=True)

"""## **Reading Data**


---Iris from https://archive.ics.uci.edu/dataset/53/iris
---covert dat files to csv files oudside the colap
---add new empty row to add Columns Names
"""

print(pd.__file__)
data=pd.read_csv('iris.csv', header=None)
data.rename(columns={0: 'sepal_length', 1: 'sepal_width',2:'petal_length',3:'petal_width',4:'class'}, inplace=True) #add Header to Columns
data.to_csv('iris_with_header.csv', index=False) # save to new csv file with Header

data.columns

data.shape

data.head()

# To describe the fields types and ensure that non-values are exists and all input data contains numeric values
data.info()
#columnstypes=dict(data.dtypes) # we use this when the number of columns is very large
#columnstpes

# To find out the number of null values **if there are null values** because empty values will affect the results.
datanull=dict(data.isna().sum())
datanull

data['class'].value_counts()
# We have three classes so we can use logistic regression

"""## **Scatter Plot**

---To show how does data look like.


"""

sns.FacetGrid(data,hue="class",height=4).map(plt.scatter,"petal_length","sepal_width").add_legeng()

"""## **Logistic Regression:**
---Converting Categorical variables into numbers:
"""

data['class']=data['class'].replace({'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2})
data.head()

data['class'].value_counts()

"""## **Preparing inputs and outputs data:**


"""

x=data.drop(["class"],axis=1)
x

y=data[['class']].values
y

data

"""## **Visualization**"""

def KPlot(feature) :
    global data
    fig, ax = plt.subplots(figsize=(10,4))
    sns.kdeplot(data[feature], shade=True)

KPlot('sepal_length')
KPlot('sepal_width')
KPlot('petal_length')
KPlot('petal_width')

"""## **Building Model**"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=44, shuffle =True)
#Splitting data
print('x_train shape is ' , x_train.shape)
print('x_test shape is ' , x_test.shape)
print('y_train shape is ' , y_train.shape)
print('y_test shape is ' , y_test.shape)

"""##**Random Forest, Bayesian, Decision Tree, Neural Networks**"""

#Random Forest library
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
#Baysian Library
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
#Decision Tree library
from sklearn.tree import DecisionTreeClassifier
##Neural Networks library
from sklearn.neural_network import MLPClassifier
##classification report library
from sklearn.metrics import classification_report
##confusion matrix library
from sklearn.metrics import confusion_matrix

GaussianNBModel,MultinomialNBModel,BernoulliNBModel = GaussianNB(), MultinomialNB(alpha=1.0), BernoulliNB(alpha=1.0,binarize=1)
RandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=300,max_depth=7,random_state=33)
DecisionTreeClassifierModel = DecisionTreeClassifier(criterion='gini',max_depth=3,random_state=33)
MLPClassifierModel = MLPClassifier(activation='tanh', # can be also identity , logistic , relu
                                   solver='lbfgs',  # can be also sgd , adam
                                   learning_rate='constant', # can be also invscaling , adaptive
                                   early_stopping= False,
                                   alpha=0.0001 ,hidden_layer_sizes=(100, 3),random_state=33)

Models = [GaussianNBModel,MultinomialNBModel,BernoulliNBModel
          ,RandomForestClassifierModel
          ,DecisionTreeClassifierModel
          ,MLPClassifierModel]

ModelsScore = {}
# for all models above
for Model in Models:
    print(f'for Model {str(Model).split("(")[0]}')
    Model.fit(x_train, y_train)

    #Calculating details
    print(f'Train Score is : {Model.score(x_train, y_train)}')
    print(f'Test Score is : {Model.score(x_test, y_test)}')

    if Model==MLPClassifierModel:
      print('Model loss is : ' , Model.loss_)
      print('Model No. of iterations is : ' , Model.n_iter_)
      print('Model No. of layers is : ' , Model.n_layers_)
      print('Model last activation is : ' , Model.out_activation_)


    #Calculating Prediction
    y_pred = Model.predict(x_test)
    if Model==MLPClassifierModel:
      y_pred_prob = Model.predict_proba(x_test)
      print('Predicted Value for Model is : ' , y_pred[:10])
      print('Prediction Probabilities Value for Model is : ' , y_pred_prob[:10])

    #y_test #Actual values

    # To compare actual and predict valuse
    '''
    compare_results=[]
    for Pred,Actual in zip(y_pred,y_test):
      if Pred == Actual:
        compare_results.append('Correct')
      else:
        compare_results.append('InCorrect')
      pd.Series(compare_results).value_counts()
    '''
    #Classification Report
    ClassificationReport = classification_report(y_test,y_pred)
    print('Classification Report is : \n', ClassificationReport )
    print(f'Precision value is  : {ClassificationReport.split()[19]}')
    print(f'Recall value is  : {ClassificationReport.split()[20]}')
    print(f'F1 Score value is  : {ClassificationReport.split()[21]}')
    ModelsScore[str(Model).split("(")[0]] = [ClassificationReport.split()[19],
                                             ClassificationReport.split()[20],ClassificationReport.split()[21]]
    #Calculating Confusion Matrix
    CM = confusion_matrix(y_test, y_pred)
    print('Confusion Matrix is : \n', CM)
    # drawing confusion matrix
    sns.heatmap(CM, center = True,cmap='Blues_r')
    plt.show()
    print('=================================================')

"""## **From the above results it is clear that the (RandomForestClassifier Algorithm is the best**, followed by (MultinomialNB Algorithm)."""